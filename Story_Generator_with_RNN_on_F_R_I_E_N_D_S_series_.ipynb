
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Story Generator with RNN on F.R.I.E.N.D.S series  .ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNZ/QwoCh/vc2MJF2khTIoq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nasiksami/Deep-Learning/blob/main/Story_Generator_with_RNN_on_F_R_I_E_N_D_S_series_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ_s1CUn4QXZ"
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/content/script/Friends_Transcript.txt'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLSiJ3Fu6Q6A"
      },
      "source": [
        "import tensorflow as tf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSMwr-6-6uhX"
      },
      "source": [
        "friends = '/content/script/Friends_Transcript.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWshy0Pe6xEl",
        "outputId": "bf543a24-61e3-4bcd-c10a-2c380e9dd21d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "text = open(friends, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# length of text is the number of characters in it\n",
        "\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 4899189 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwRAgOWM7Jub"
      },
      "source": [
        "#script =friends.read() #read the script file\n",
        "import re\n",
        "pattern = re.compile(r'\\s(?=\\w+(?=:))') # store the regex\n",
        "result = re.split(pattern, text) # split the script where our pattern matched (pink dot)\n",
        "\n",
        "for item in result:\n",
        "    split_line = item.split(':')\n",
        "    try:\n",
        "        character = split_line[0]\n",
        "        speech = split_line[1]\n",
        "    except:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Krq3L5XI76IO",
        "outputId": "b220c01d-3274-40a5-b290-31f6934aad38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "THE ONE WHERE MONICA GETS A NEW ROOMATE (THE PILOT-THE UNCUT VERSION)\n",
            "Written by: Marta Kauffman & David Crane\n",
            "[Scene: Central Perk, Chandler, Joey, Phoebe, and Monica are there.]\n",
            "Monica: There's nothing to tell! He's just some guy I work with!\n",
            "Joey:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Z3b3c9r8wZd",
        "outputId": "85188f59-b16d-44c8-9782-17ff2a8be148",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# The unique characters in the file\n",
        "\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "94 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9Fbaq9-8zOX"
      },
      "source": [
        "# process the text\n",
        "\n",
        "char2index = {u:i for i, u in enumerate(vocab)}\n",
        "index2char = np.array(vocab)\n",
        "text_as_int = np.array([char2index[c] for c in text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX4EiezM81gb",
        "outputId": "4f805046-3f75-43e4-a6a7-dd2d36c2096a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2index, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2index[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '\"' :   3,\n",
            "  '#' :   4,\n",
            "  '$' :   5,\n",
            "  '%' :   6,\n",
            "  '&' :   7,\n",
            "  \"'\" :   8,\n",
            "  '(' :   9,\n",
            "  ')' :  10,\n",
            "  '*' :  11,\n",
            "  '+' :  12,\n",
            "  ',' :  13,\n",
            "  '-' :  14,\n",
            "  '.' :  15,\n",
            "  '/' :  16,\n",
            "  '0' :  17,\n",
            "  '1' :  18,\n",
            "  '2' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThqJJfHB87Fq",
        "outputId": "7a3c4681-3a35-4f7e-b549-24f08db657aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'THE ONE WHERE' ---- characters mapped to int ---- > [53 41 38  1 48 47 38  1 56 41 38 51 38]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH3CD0FV89l7",
        "outputId": "0fe89699-0ed0-445f-eb6a-b822108afede",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# convert text vector to charcter indices\n",
        "\n",
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "      print(index2char[i.numpy()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "T\n",
            "H\n",
            "E\n",
            " \n",
            "O\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5lTo-LC9CEn",
        "outputId": "a1256d26-198b-4689-fc2c-aa29583f5090",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# convert character to sequence\n",
        "\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "    print(repr(''.join(index2char[item.numpy()])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'THE ONE WHERE MONICA GETS A NEW ROOMATE (THE PILOT-THE UNCUT VERSION)\\nWritten by: Marta Kauffman & Da'\n",
            "\"vid Crane\\n[Scene: Central Perk, Chandler, Joey, Phoebe, and Monica are there.]\\nMonica: There's nothin\"\n",
            "\"g to tell! He's just some guy I work with!\\nJoey: C'mon, you're going out with the guy! There's gotta \"\n",
            "'be something wrong with him!\\nChandler: All right Joey, be nice. So does he have a hump? A hump and a '\n",
            "\"hairpiece?\\nPhoebe: Wait, does he eat chalk?\\n(They all stare, bemused.)\\nPhoebe: Just, 'cause, I don't \"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QUypI6i9Goz"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOwSzmf19JrN",
        "outputId": "f0a67a62-9fdb-4bb8-f4cf-e726b170c263",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# print input and target values\n",
        "\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "    print ('Input data: ', repr(''.join(index2char[input_example.numpy()])))\n",
        "    print ('Target data:', repr(''.join(index2char[target_example.numpy()])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'THE ONE WHERE MONICA GETS A NEW ROOMATE (THE PILOT-THE UNCUT VERSION)\\nWritten by: Marta Kauffman & D'\n",
            "Target data: 'HE ONE WHERE MONICA GETS A NEW ROOMATE (THE PILOT-THE UNCUT VERSION)\\nWritten by: Marta Kauffman & Da'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhoXjtLj9LS9",
        "outputId": "c2fbb910-adb5-49c3-e9a5-e96cbfabe9a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuaBafGO9O85"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 128\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT8gT24c-R3u",
        "outputId": "c5b09de9-7b55-45f0-96f6-f83a17574d82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# build the model\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[BATCH_SIZE, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 128)           12032     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 256)           296448    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 94)            24158     \n",
            "=================================================================\n",
            "Total params: 332,638\n",
            "Trainable params: 332,638\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2dl5JdZ-Ulg",
        "outputId": "b2db74d6-4453-41dc-f7ff-c7dc1918aeee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 94) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McCn_iFO-X21",
        "outputId": "df2eba9f-a77d-4c16-b0b4-db45b95794ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 94)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.54306\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJldxvVk-cEq"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAIwwk8Y-g-j"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGZ8sNk8-jBl",
        "outputId": "b81ac2a9-0096-4c3a-de75-c51629be18fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "history = model.fit(dataset, epochs=10, callbacks=[checkpoint_callback])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "757/757 [==============================] - 15s 19ms/step - loss: 2.0516\n",
            "Epoch 2/10\n",
            "757/757 [==============================] - 15s 19ms/step - loss: 1.4549\n",
            "Epoch 3/10\n",
            "757/757 [==============================] - 14s 19ms/step - loss: 1.3384\n",
            "Epoch 4/10\n",
            "757/757 [==============================] - 15s 20ms/step - loss: 1.2879\n",
            "Epoch 5/10\n",
            "757/757 [==============================] - 15s 19ms/step - loss: 1.2585\n",
            "Epoch 6/10\n",
            "757/757 [==============================] - 15s 19ms/step - loss: 1.2382\n",
            "Epoch 7/10\n",
            "757/757 [==============================] - 15s 19ms/step - loss: 1.2239\n",
            "Epoch 8/10\n",
            "757/757 [==============================] - 14s 19ms/step - loss: 1.2126\n",
            "Epoch 9/10\n",
            "757/757 [==============================] - 14s 19ms/step - loss: 1.2033\n",
            "Epoch 10/10\n",
            "757/757 [==============================] - 14s 19ms/step - loss: 1.1959\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgW7exWS-k2P",
        "outputId": "e0dea19c-df81-4617-b661-416ebe19f54d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# to keep prediction step simple we use batch size of 1. for that we need to rebuild model\n",
        "# and restore weight from checkpoint\n",
        "\n",
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./training_checkpoints/ckpt_10'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eem8JWL5-uWk",
        "outputId": "b40636c8-38ae-406c-c520-42ead35803eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# rebuild the model\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[1, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "])\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (1, None, 128)            12032     \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (1, None, 256)            296448    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (1, None, 94)             24158     \n",
            "=================================================================\n",
            "Total params: 332,638\n",
            "Trainable params: 332,638\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etMtHb6L-zZ-"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "    # Number of characters to generate\n",
        "    num_generate = 5000\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2index[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    temperature = 1.0\n",
        "\n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "        # We pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(index2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zVURy9--1Jk",
        "outputId": "4c25f64c-9474-4379-f434-205f42a0c162",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"Chandler: \"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chandler: (entering). I didn't tell you, tell me anything to make since I could make a crush out, moms ideas?\n",
            "Phoebe: I never totally never haft the morning! Ffoctomst, I mean you know what? I'm so youre a lovers! (Ross and Phoebe smite officals interesting.\n",
            "Monica: Come on! You are so great!\n",
            "Phoebe: Count The Woman, the face and, and there's really like that...E memony like how memballs ment that woman to meet you Dut sitting in versian!\n",
            "Ross: Oh... you know what? \n",
            "Joey to hold the way the camera.)\n",
            "Gavinita: I'm tyoing careful and hes slowly looking get to stay center early no furlotming a pause> Live it I know those kind, I don't know what I see me... holding about this? (you ware! (Kathy phone) Hmmm... (checks it upsure in it!)\n",
            "Rachel: Ok ya-2-7-7.\n",
            "[Scene: Central Perk. Monica is on a posion beach tocan re like to help your mom. (Listens) Comportant, how wow! (She go-two houses.)\n",
            "Ross: No, no, no, no! Not! Thats atricf cut so he should tell ...Mast let me feel birt.\n",
            "Joey: Yeah-ho-ho...She, we were a sezuarly into since we just grow that Tod.\n",
            "Ross: Hmmm, Im gonna got there a polln!! I have too! And now youre blind it now.\n",
            "Mr. Mike: Well, this is fungy thing okay! This is akah....\n",
            "Phoebe: All right, le some disparanning for him, were ok and we got a baby in sat hyms inergedally. I could have staring ago away for all of you? \n",
            "Joey: Oh yeah, youve got to be parents off and we're returning in the mommous Ross about this memelling her.\n",
            "Rachel: Whats rule all the numbers. Chandler and Chandler are sitting thinkin' of thing they do where that is shaking did the game a songh, and this is not very...\n",
            "Barca: Get out to a part didn't like the air, she is gonna live to you when you talk to dinner.\n",
            "(Chandler does so someone don't like that round like, we gotta not can spen ...\n",
            "Phoebe: Ok thank you.\n",
            "Eric: I celled talk as getting really like!\n",
            "Rachel: Oh my God! You oup officed's Okay? (Enter) Foon along.\n",
            "Rachel: Oh.\n",
            "Ofica: I read a little job tor little boy, I was gonna   stole to say to a stop!\n",
            "Monica: Shhh, there is a party (picks up there on The One Williny date later on anyway, Ross is really underward   on You know a glood-out.\n",
            "Chandler: I think well...\n",
            "Monica: Hmm!\n",
            "Rachel: Hi!\n",
            "Phoebe: Well, no. I don't believe the time that would Chandler gets just a bralethe just being celebration of office.) (looks down.) Yeah, on That share the's still check thank you.\" Backpackline) Hey Bel: Why dont you do that?\n",
            "Joey: Yeah, wh-where Eum-cow!\n",
            "Chandler: Well, because You!\n",
            "Chandler: (de bottlichen my Days? (Pause) Oh, Thankler: Okay, okay.\n",
            "Tag: Dr. Green! Are you off.\n",
            "Ross: You guys er - guy-arrifose.\n",
            "Phoebe: What happened) Are you!! (He starts talking into down the this apartmas I think me this; tip in your opened)\n",
            "Monica: You a seat. You know.! Thats giving around very good ning grazy, do hel my pack and I talked to Chandler, you should's goin'in the baby wash.\n",
            "(the best and do the apartment. is you, you know, I blabbed exbelieven voice! We got to have later bag where?\n",
            "Phoebe: You're a lot of chickearw.\n",
            "Monica: (hearting to say he who do this reallweddless?\n",
            "Monica: Well, I get at the women.\n",
            "Joey: Oh come on, no, no, you dont think that was remember sex cusuingavinial half can come to see it comes to the door, and she takes the feart   offered without right. It's Joey called. And I have to get us.\n",
            "Rachel: You're here so much load!   Phoebe on the streeves.\n",
            "Roy: Hey, I just have to stand in your birthday! Rachel has to get rubbing him nervous.\n",
            "Rachel: I took our! (thinks)\n",
            "Rachel: You can't makes your button in the birthdayber: Hey! I grabe going.\n",
            "Chandler: I think she has thinking of my mist!\n",
            "(Joeys loom) Hey! Hey! Choos! Thats right.\n",
            "Chandler: Stayen! You care what happened? (She starts.)\n",
            "Joey: Ooh! We should go before anything. I think you know what Im just filled, see Monica\n",
            "Ross: Yeah.\n",
            "Rachel: (Listens) You gotta get out. You because you're always hand trest like a show holding her over theres on the recorder month you could preten: (looks in the outsids very impaure out from a grints.) Oooh, alright, well, Main Ashmanks when they really should get it! Not as slapping over again.]\n",
            "Rachel: Well, you know.\n",
            "Chandler: Yay!\n",
            "Chandler: Great! Sers what to do out the hays!\n",
            "Ross: Yeah? (to Rachel) Why?!\n",
            "Phoebe: I get them all as Chandler problems around she looks over a little good one, on tog and specish that she walks people.]\n",
            "Monica: Hey!\n",
            "Rachel: Bye gir class!\n",
            "Phoebe: Ok, okay... crack that's freaks.\n",
            "Monica: Okay.\n",
            "Ross: I's it up.\n",
            "Joey: I dont sure you gonna dead exclastical verock quiet Christmachel: Oh, yeah! Hi-Phoebe!\n",
            "(Janice enters.)\n",
            "[The man. let me a way I just really do it!\n",
            "Joey: (at the rest of the head-is the roll things. (Ross listens.) Has are a great grarically: (to Ross) That's not a relation.\n",
            "Joey: Oh, you keeps even turn!\n",
            "Janice: Yeah!\n",
            "Rachel: Right, he's not extratters I sat dogs... Wheres Richard!\n",
            "Rachel: (as Richard) Oh yeah. Okay, lat ex!\n",
            "Phoebe: Phoebe returns crying]\n",
            "Tells, Pheebs, you're gonna be our else, I can't b\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxhXoh9P_Xoi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud8_IHFeAdKM"
      },
      "source": [
        "LETS TRY WITH LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKqkQ05EAlHH"
      },
      "source": [
        "#Encoding\n",
        "#convert it into integer. All the characters.\n",
        "\n",
        "vocab = sorted(set(text))  #to sort all the unique characters\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}  #mapping\n",
        "idx2char = np.array(vocab) #making list or array to use the indexes\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text]) #convert every single characters from the text into their integer representation \n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsisj1lQAqxO",
        "outputId": "c97b35a7-2a6b-4475-c571-35f6f46cc942",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# lets look at how part of our text is encoded\n",
        "print(\"Text:\", text[:13])\n",
        "print(\"Encoded:\", text_to_int(text[:13]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text: THE ONE WHERE\n",
            "Encoded: [53 41 38  1 48 47 38  1 56 41 38 51 38]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NbTs7mXAt_K",
        "outputId": "ffb1ad69-f86f-4fb3-b4a9-03fd622ca72a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy() #if its not already a numpy array, it will make it in numpy array\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:133]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "THE ONE WHERE MONICA GETS A NEW ROOMATE (THE PILOT-THE UNCUT VERSION)\n",
            "Written by: Marta Kauffman & David Crane\n",
            "[Scene: Central Perk, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yH7Ah1BAxk_"
      },
      "source": [
        "#training\n",
        "#whatever input we have, we will train in such a way that the output is one character ahead\n",
        "#for example We will Input Cana the O/P will be anad\n",
        "seq_length = 100  # length of sequence for a training example\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int) #converts string to characters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf8NGZJuA4gf"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9HTHi9YA6O-"
      },
      "source": [
        "#split input target\n",
        "def split_input_target(chunk):  # for the example: hello\n",
        "    input_text = chunk[:-1]  # hell\n",
        "    target_text = chunk[1:]  # ello\n",
        "    return input_text, target_text  # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRPcF8TBA8MM",
        "outputId": "69c94456-82ff-4c1f-c9be-519f8da50e06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "THE ONE WHERE MONICA GETS A NEW ROOMATE (THE PILOT-THE UNCUT VERSION)\n",
            "Written by: Marta Kauffman & D\n",
            "\n",
            "OUTPUT\n",
            "HE ONE WHERE MONICA GETS A NEW ROOMATE (THE PILOT-THE UNCUT VERSION)\n",
            "Written by: Marta Kauffman & Da\n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "vid Crane\n",
            "[Scene: Central Perk, Chandler, Joey, Phoebe, and Monica are there.]\n",
            "Monica: There's nothi\n",
            "\n",
            "OUTPUT\n",
            "id Crane\n",
            "[Scene: Central Perk, Chandler, Joey, Phoebe, and Monica are there.]\n",
            "Monica: There's nothin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHr6QkLzA92w"
      },
      "source": [
        "#Batch here means 64 different sequences of training examples\n",
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLQ4ivNNBFQC"
      },
      "source": [
        "Building the Model\n",
        "\n",
        "Now it is time to build the model. We will use an embedding layer a LSTM and one dense layer that contains a node for each unique character in our training data. The dense layer will give us a probability distribution over all nodes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3lDC_QuBDCS",
        "outputId": "9688bd16-a49a-452f-f6e1-36e59e83c004",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (64, None, 256)           24064     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (64, None, 94)            96350     \n",
            "=================================================================\n",
            "Total params: 5,367,390\n",
            "Trainable params: 5,367,390\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrfmxXf9BLc4",
        "outputId": "9562eafa-4a21-4048-e582-59cb83d08a59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 94) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXBOk3tSBQ0Z",
        "outputId": "85f64ecd-9c94-4e0e-d2b9-1a982dda131d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[-2.73887534e-03 -7.74494046e-03  2.20756093e-03 ...  2.57569831e-03\n",
            "    1.33047835e-03 -2.62582838e-03]\n",
            "  [-2.74408446e-03 -2.49624532e-03  4.26726742e-03 ...  7.97169283e-04\n",
            "    5.12999203e-03  2.36386375e-04]\n",
            "  [-3.34032625e-03 -1.25609303e-03  5.14965830e-03 ...  2.96361279e-03\n",
            "    9.94541962e-03  1.88797712e-04]\n",
            "  ...\n",
            "  [ 3.49248154e-03  4.98982787e-04  5.17737819e-04 ... -4.29233164e-03\n",
            "   -6.81733154e-03 -4.08193097e-03]\n",
            "  [-1.44741393e-03  2.05711578e-03  2.24246364e-03 ... -1.59115309e-03\n",
            "   -8.24465230e-03 -6.46302011e-03]\n",
            "  [ 4.95924754e-03  5.02006523e-03 -4.56887903e-03 ... -1.93669507e-03\n",
            "   -3.04073468e-03 -8.33479315e-03]]\n",
            "\n",
            " [[-4.86154482e-03 -1.41304731e-02  9.23144352e-03 ...  3.15065216e-03\n",
            "   -9.40760225e-03  5.68275340e-03]\n",
            "  [-4.17194236e-03 -1.05023384e-02  4.33431473e-03 ... -1.78608682e-03\n",
            "   -1.12338988e-02  5.69357537e-03]\n",
            "  [-3.24340817e-03 -1.28085008e-02  8.61454383e-03 ... -4.20396682e-03\n",
            "   -5.60572091e-03  7.18792574e-03]\n",
            "  ...\n",
            "  [ 3.65926791e-03 -9.96952178e-04 -3.85719584e-03 ...  1.93736609e-03\n",
            "   -4.28082980e-03 -6.85026916e-03]\n",
            "  [ 5.38233947e-03  1.63878000e-03 -7.33267050e-04 ... -6.50369329e-03\n",
            "   -3.83371394e-03 -5.99749107e-03]\n",
            "  [ 4.09056433e-04  4.70620580e-06  2.44040973e-04 ... -9.00502317e-03\n",
            "   -3.29262624e-03 -4.89277858e-03]]\n",
            "\n",
            " [[-5.49610239e-03 -1.77108194e-03  1.18367672e-02 ...  1.00000994e-04\n",
            "   -2.82555632e-03 -3.17432801e-03]\n",
            "  [-5.90354204e-04 -8.61183391e-04  1.25402659e-02 ...  1.56709645e-03\n",
            "   -2.70540453e-03 -5.11452509e-03]\n",
            "  [-2.84764147e-03 -1.67656282e-03  6.05163723e-03 ... -3.66828591e-03\n",
            "   -6.10654149e-03 -2.61266110e-03]\n",
            "  ...\n",
            "  [ 5.84499957e-03 -2.92431959e-03 -3.50398384e-03 ...  5.93159068e-03\n",
            "    3.77599942e-03 -3.78565537e-03]\n",
            "  [ 4.08016890e-03 -1.06286593e-02  7.92945502e-04 ...  2.16488773e-03\n",
            "    4.16271109e-03 -4.10991721e-04]\n",
            "  [ 5.44577511e-03 -6.28131349e-03  9.89979366e-04 ...  5.44306962e-03\n",
            "    4.96414723e-03  1.67102367e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-5.41698840e-03 -1.11527359e-02  6.17588591e-03 ... -4.05833591e-03\n",
            "    3.63046909e-03  1.27033331e-04]\n",
            "  [ 2.15985090e-03 -4.58889967e-03 -1.74046727e-03 ... -1.75300590e-03\n",
            "    3.60249402e-03 -5.12582995e-03]\n",
            "  [-5.89874014e-03 -6.33564871e-03 -1.39936595e-03 ...  3.73219745e-03\n",
            "    3.08185280e-03 -8.91457684e-03]\n",
            "  ...\n",
            "  [-5.16767194e-03 -3.46744945e-03  9.94726829e-03 ... -2.59613851e-03\n",
            "   -3.71798221e-03  3.90276965e-03]\n",
            "  [ 2.19660485e-03  3.69588844e-04  2.70108180e-03 ... -1.54330034e-03\n",
            "   -1.69542572e-03  1.47430657e-03]\n",
            "  [ 6.74181618e-04  2.56277504e-04  4.31044307e-03 ...  5.56399347e-04\n",
            "    5.16497856e-03  1.14446040e-03]]\n",
            "\n",
            " [[-8.96598026e-03 -7.86975585e-03  7.81833287e-03 ... -8.32030084e-03\n",
            "   -5.40656829e-03  2.73639360e-03]\n",
            "  [-7.71293696e-03 -1.25436876e-02  1.12202587e-02 ... -9.11374483e-03\n",
            "   -2.39284849e-03  5.40036429e-03]\n",
            "  [-7.25069735e-03 -1.35689918e-02  1.27954325e-02 ... -4.52374108e-03\n",
            "   -1.85375696e-03  6.28990936e-04]\n",
            "  ...\n",
            "  [-6.21857960e-03  2.26021488e-03  1.54708605e-03 ... -1.61197968e-02\n",
            "   -1.13682561e-02 -1.80405891e-03]\n",
            "  [-7.83517957e-03  1.99512392e-03 -5.55616803e-04 ... -1.45980511e-02\n",
            "   -1.35156941e-02 -1.39591470e-03]\n",
            "  [-7.29536079e-03  1.98169006e-03  4.11493331e-03 ... -7.92879052e-03\n",
            "   -1.12344306e-02 -7.08616897e-03]]\n",
            "\n",
            " [[-3.34995124e-03 -1.95478369e-02  7.20051117e-03 ...  2.92753521e-03\n",
            "    3.01210210e-03  7.72886910e-04]\n",
            "  [-2.87101022e-03 -1.37770344e-02  1.24003999e-02 ...  4.39572986e-03\n",
            "    1.77673413e-03  5.39998757e-04]\n",
            "  [-3.71290976e-03 -1.00343423e-02  1.09054912e-02 ...  5.24518825e-03\n",
            "    9.30179656e-03 -5.76968770e-04]\n",
            "  ...\n",
            "  [-4.78052581e-03 -8.78546946e-03  1.65169640e-03 ...  3.75160656e-04\n",
            "   -8.92067794e-04  4.28716233e-03]\n",
            "  [ 3.75607330e-03 -8.26816913e-03 -5.14850719e-04 ...  4.06211615e-03\n",
            "    1.88636128e-03  4.81598545e-03]\n",
            "  [ 2.07959581e-03 -4.56599006e-03  1.73632591e-03 ...  6.21407526e-03\n",
            "    9.05378442e-03  1.48062780e-03]]], shape=(64, 100, 94), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3EP1W_RBRAZ",
        "outputId": "cf995415-2e76-42bb-e76a-ec1d84d8404e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# lets examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[-0.00273888 -0.00774494  0.00220756 ...  0.0025757   0.00133048\n",
            "  -0.00262583]\n",
            " [-0.00274408 -0.00249625  0.00426727 ...  0.00079717  0.00512999\n",
            "   0.00023639]\n",
            " [-0.00334033 -0.00125609  0.00514966 ...  0.00296361  0.00994542\n",
            "   0.0001888 ]\n",
            " ...\n",
            " [ 0.00349248  0.00049898  0.00051774 ... -0.00429233 -0.00681733\n",
            "  -0.00408193]\n",
            " [-0.00144741  0.00205712  0.00224246 ... -0.00159115 -0.00824465\n",
            "  -0.00646302]\n",
            " [ 0.00495925  0.00502007 -0.00456888 ... -0.0019367  -0.00304073\n",
            "  -0.00833479]], shape=(100, 94), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phhIMplkBPqM",
        "outputId": "66da537d-9fc6-4cbb-d095-88555937252f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# and finally well look at a prediction at the first timestep\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# and of course its 65 values representing the probabillity of each character occuring next"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "94\n",
            "tf.Tensor(\n",
            "[-2.7388753e-03 -7.7449405e-03  2.2075609e-03  3.5514729e-03\n",
            "  1.3080693e-02  8.4709842e-03 -1.2235443e-02  6.8866587e-03\n",
            "  2.4864040e-03 -3.1110789e-03  1.5032735e-03 -4.9297898e-03\n",
            "  6.9309631e-03  2.4491642e-04 -6.6305096e-03 -3.3801536e-03\n",
            " -5.6461506e-03 -1.6940153e-03  4.7120955e-03 -2.3632608e-03\n",
            " -1.8994850e-03  7.5719785e-03  7.5461199e-03  3.3869823e-03\n",
            " -5.9433701e-04 -6.2652864e-04  2.7200549e-03 -1.3934993e-03\n",
            " -2.1327515e-03  2.1732820e-03  1.8564388e-03 -1.9032671e-04\n",
            "  2.9990377e-03  1.7745350e-03  7.1851769e-03 -8.4970528e-03\n",
            " -5.4296758e-03  2.8720561e-03 -1.1835566e-02  4.5500793e-03\n",
            " -1.1331175e-04 -9.4219651e-03  2.4219069e-03 -8.5970778e-03\n",
            " -4.5025405e-03 -7.2283372e-03  7.3333282e-04  6.0473252e-03\n",
            " -1.3558152e-03 -1.3976358e-05 -1.3622472e-02 -2.9441800e-03\n",
            "  5.4334006e-03  2.1101958e-03  7.1236659e-03  6.1732512e-03\n",
            " -6.1560739e-03 -5.7636038e-03  1.1134675e-02 -4.8813135e-03\n",
            "  2.0750761e-03  4.7808718e-03  1.3789454e-03 -3.6973692e-04\n",
            " -4.2965282e-03  5.5224407e-03  8.2163252e-03  2.7836936e-03\n",
            " -5.3870096e-03 -5.0209542e-03  1.6058306e-03 -4.9458817e-05\n",
            " -3.3246123e-03  6.1252862e-03  7.7718804e-03 -2.4188338e-03\n",
            " -1.1134072e-02  1.0446079e-02  2.8327983e-03  4.7397846e-03\n",
            "  3.2705953e-03 -1.7058770e-03  4.7081499e-03 -1.5952764e-04\n",
            "  2.7486302e-03  4.1404809e-03  5.0158352e-03  1.7960561e-03\n",
            "  1.5865084e-02 -1.1672234e-02 -4.0128119e-03  2.5756983e-03\n",
            "  1.3304783e-03 -2.6258284e-03], shape=(94,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBu_30QvBiHT",
        "outputId": "f92a9d34-f377-462f-e7d3-74ef098ed278",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars  # and this is what the model predicted for training sequence 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"*!*x:Y?F..6k=7zd*U{=[bEm5{5Wc;X+*mbNI>yud-X0:`6N/2Fi.L:!m:]}V9wN @E:X_4\\nX3_0'yv>s`F}#gok]#?Q%zonil(C\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFifb7F_BkLn"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxc6YmfqBnBR"
      },
      "source": [
        "#completing the model\n",
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-SOQ78FBoJ4"
      },
      "source": [
        "#creating checkpoints\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55BPBfQRBqeQ",
        "outputId": "98c7dc3d-3fac-461c-9c91-bf6732129eee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "history = model.fit(data, epochs=20, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "757/757 [==============================] - 69s 91ms/step - loss: 1.3027\n",
            "Epoch 2/20\n",
            "757/757 [==============================] - 69s 91ms/step - loss: 1.2151\n",
            "Epoch 3/20\n",
            "757/757 [==============================] - 69s 91ms/step - loss: 1.1666\n",
            "Epoch 4/20\n",
            "757/757 [==============================] - 69s 91ms/step - loss: 1.1335\n",
            "Epoch 5/20\n",
            "757/757 [==============================] - 69s 91ms/step - loss: 1.1082\n",
            "Epoch 6/20\n",
            "757/757 [==============================] - 69s 91ms/step - loss: 1.0869\n",
            "Epoch 7/20\n",
            "757/757 [==============================] - 69s 91ms/step - loss: 1.0691\n",
            "Epoch 8/20\n",
            "757/757 [==============================] - 69s 91ms/step - loss: 1.0521\n",
            "Epoch 9/20\n",
            "757/757 [==============================] - 69s 91ms/step - loss: 1.0371\n",
            "Epoch 10/20\n",
            "757/757 [==============================] - 69s 91ms/step - loss: 1.0227\n",
            "Epoch 11/20\n",
            "757/757 [==============================] - 70s 92ms/step - loss: 1.0097\n",
            "Epoch 12/20\n",
            "757/757 [==============================] - 69s 92ms/step - loss: 0.9968\n",
            "Epoch 13/20\n",
            "757/757 [==============================] - 70s 92ms/step - loss: 0.9852\n",
            "Epoch 14/20\n",
            "757/757 [==============================] - 69s 92ms/step - loss: 0.9738\n",
            "Epoch 15/20\n",
            "757/757 [==============================] - 69s 92ms/step - loss: 0.9628\n",
            "Epoch 16/20\n",
            "757/757 [==============================] - 69s 92ms/step - loss: 0.9528\n",
            "Epoch 17/20\n",
            "757/757 [==============================] - 69s 91ms/step - loss: 0.9429\n",
            "Epoch 18/20\n",
            "757/757 [==============================] - 69s 91ms/step - loss: 0.9335\n",
            "Epoch 19/20\n",
            "757/757 [==============================] - 69s 91ms/step - loss: 0.9246\n",
            "Epoch 20/20\n",
            "757/757 [==============================] - 69s 91ms/step - loss: 0.9163\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iVqIka0BsPm"
      },
      "source": [
        "#loading the model\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDcMvzXPBwzQ"
      },
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tCxZo3GByAp"
      },
      "source": [
        "#We can load any checkpoint we want by specifying the exact file to load"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyOU5p9nBzTv"
      },
      "source": [
        "checkpoint_num = 10\n",
        "#model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvGkEc55B1xz"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1500\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "    \n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciXCvkl6B3wL",
        "outputId": "dd524b5e-95da-4136-f8a7-eb35ab030a54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "print(generate_text(model, inp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type a starting string: chandler\n",
            "chandler, get it! (She gives him the picture.)\n",
            "CHANDLER: Wha...  EDDIE: Is some nice kitchen? \n",
            "RACHEL: You know, Ross. Ross, you just don't mean about how smart knowing it is about me, everythings hit by (Ross tackles the slive on the seat.) Okay, scohing to Monica) What locked me?! (He's leaving with her purse and smiles) Ah, oh, uh yeah, there's all for her, uhm... I have   jokes. Uh, I was a waitress.  So I can fit into private ruined.\n",
            "Chandler: I can sprave that strings wed face for Phoebe. Phoebe the twins and stuck to guestily.) Well then, Im so sorry about this the hardware-sawce! So anyway we seen him in the hall, but it makes you feel now, but lets just  \n",
            "Joey: Chandler? (Monica looks convincing near the set) Not every weird hair to see the cookie.) Ooh! Now I'm steaming, and if its an opportunity test I went through a wedding actor with no resum, kissing Charlie and a family to put speech.]\n",
            "Chandler: Honey, that's the smell of secret opportunity. (Motions to the window)\n",
            "Announcer: Seriously hes coming, but I wanted to see a   regatagett married. Was I like the money?\n",
            "Phoebe: You must be impressed. (Holds up one.) Uh, Rach, I-I heard it. (To Rachel) Let me ask you something! Drunk as me and I was rolling for a wall.\n",
            "Ross: You know, I'd like to take you out.\n",
            "Phoebe: Okay. Phoebe that actually inside is why   huge, cause, you know, that is a big presentation in the pool of the altheir clown. It's standing conseltion to sad. I'm surprised to make you up alone, I'm going to get \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHa6PIoeIw8z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
